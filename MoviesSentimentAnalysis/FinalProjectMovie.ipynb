{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'test.tsv.zip', 'train.tsv', 'train.tsv.zip']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ntlk libraries for disctionaries and word_tokenization while preprocessing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "import re \n",
    "\n",
    "#tqdm for progress bar visuals during training\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F #for one-hot encoding of labels\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Libraries for final tokenization of data into numerical tensors for training\n",
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Training utilities\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#For dataset exctraction\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for training and testing exctraction from base directory\n",
    "train= pd.read_csv(\"./input/train.tsv\", sep=\"\\t\")\n",
    "test = pd.read_csv(\"./input/test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>156061</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>156062</td>\n",
       "      <td>8545</td>\n",
       "      <td>An intermittently pleasing but mostly routine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>156063</td>\n",
       "      <td>8545</td>\n",
       "      <td>An</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156064</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>156065</td>\n",
       "      <td>8545</td>\n",
       "      <td>intermittently pleasing but mostly routine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase\n",
       "0    156061        8545  An intermittently pleasing but mostly routine ...\n",
       "1    156062        8545  An intermittently pleasing but mostly routine ...\n",
       "2    156063        8545                                                 An\n",
       "3    156064        8545  intermittently pleasing but mostly routine effort\n",
       "4    156065        8545         intermittently pleasing but mostly routine"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head() #Check test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head() #Check train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "\n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        if isinstance(sent, str):\n",
    "            review_text = BeautifulSoup(sent, 'html.parser').get_text()#Remove html content\n",
    "            review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)#Remove non-alphabetic characters\n",
    "            words = word_tokenize(review_text.lower())#Word tokenize the sentences\n",
    "            lemma_words = [lemmatizer.lemmatize(i) for i in words]#Lemmatize each word to its lemma\n",
    "\n",
    "            reviews.append(lemma_words)\n",
    "        else:\n",
    "            #Handle non-string values\n",
    "            reviews.append([])\n",
    "\n",
    "    return reviews #return list of preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# Download the 'punkt' package\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "    \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█                                                                         | 2330/156060 [00:01<01:05, 2338.58it/s]C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2188\\811434151.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  review_text = BeautifulSoup(sent, 'html.parser').get_text()#Remove html content\n",
      "100%|████████████████████████████████████████████████████████████████████████| 156060/156060 [00:25<00:00, 6208.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 66292/66292 [00:09<00:00, 6839.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060\n",
      "66292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#clean sentences and check their length\n",
    "train_sentences = clean_sentences(train)\n",
    "test_sentences = clean_sentences(test)\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor(train.Sentiment.values) \n",
    "#Calculate the number of classes\n",
    "num_classes = torch.max(target) + 1\n",
    "#Convert to one-hot encoding\n",
    "y_target = F.one_hot(target, num_classes=num_classes)\n",
    "#Convert to float tensor as it is used for optimizer\n",
    "y_target = y_target.to(torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.20,stratify=y_target, random_state = 40)\n",
    "#Split dataset into training and validation data\n",
    "#Validation data is needed for KFold validation and early stopping techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 124848/124848 [00:00<00:00, 594166.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13730\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#It is needed for initializing tokenizer of keras and subsequent padding\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "for sent in tqdm(X_train):\n",
    "    \n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max = len(sent)\n",
    "        \n",
    "#Length of the list of unique_words gives the no of unique words\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 48) (31212, 48) (66292, 48)\n"
     ]
    }
   ],
   "source": [
    "#Tokenize and then convert to equal-sized data\n",
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tensors inputs for pytorch nerual network\n",
    "X_test = torch.tensor(X_test)\n",
    "X_train = torch.tensor(X_train)\n",
    "X_val = torch.tensor(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([124848, 48]) torch.Size([31212, 48]) torch.Size([66292, 48])\n"
     ]
    }
   ],
   "source": [
    "#Check shapes\n",
    "print(X_train.shape,X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One of the layers = embedding needed for dataset of words training\n",
    "#Dropout layer is reqularization technique to drop excessive outputs\n",
    "\n",
    "#Simple LSTM model with two layers\n",
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1,output_dim, dropout):\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dim1, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out1, _ = self.lstm1(embedded)\n",
    "        fc1_out = self.fc1(lstm_out1[:, -1, :])\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        output = self.softmax(fc1_out)\n",
    "        return output\n",
    "    \n",
    "#Complex LSTM model with 4 layers\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, hidden_dim2, output_dim, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim1, hidden_dim2, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.fc1 = nn.Linear(hidden_dim2, 100)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(100, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out1, _ = self.lstm1(embedded)\n",
    "        lstm_out2, _ = self.lstm2(lstm_out1)\n",
    "        lstm_out2 = lstm_out2[:, -1, :]\n",
    "        fc1_out = self.fc1(lstm_out2)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        output = self.fc2(fc1_out)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "    \n",
    "#Complex LSTM model with 3 layers, 1 lstm and 2 linear layers\n",
    "class ModerateLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1, output_dim, dropout):\n",
    "        super(ModerateLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim1, dropout=dropout, batch_first=True, bidirectional=False)\n",
    "        self.fc1 = nn.Linear(hidden_dim1, 100)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(100, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out1, _ = self.lstm1(embedded)\n",
    "        lstm_out1 = lstm_out1[:, -1, :]\n",
    "        fc1_out = self.fc1(lstm_out1)\n",
    "        fc1_out = self.dropout(fc1_out)\n",
    "        output = self.fc2(fc1_out)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "# Convert data to PyTorch DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From input data create loaders\n",
    "def create_data_loader(X, y):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) #Batch size 32\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***KFOLD VALIDATION FOR DIFFERENT LEARNING RATES***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_lstm_lr(a_model):\n",
    "    #use different learning rates for comparison\n",
    "    learning_rates = [0.0005, 0.0003, 0.0001]\n",
    "    device = \"cpu\"\n",
    "    num_epochs = 8\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    all_train_losses = []  \n",
    "    all_val_losses = []    \n",
    "    all_train_f1_scores = []  \n",
    "    all_val_f1_scores = []    \n",
    "    early_stopping_patience = 2  \n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"Testing Learning Rate: {lr}\")\n",
    "\n",
    "        val_accuracies = []\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(X_train)):\n",
    "\n",
    "            model = a_model\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "            #Split data by specific folds\n",
    "            X_train_fold, y_train_fold = X_train[train_indices], y_train[train_indices]\n",
    "            X_val_fold, y_val_fold = X_train[val_indices], y_train[val_indices]\n",
    "\n",
    "            early_stopping_counter = 0\n",
    "            \n",
    "            train_loader = create_data_loader(X_train_fold, y_train_fold)\n",
    "            val_loader = create_data_loader(X_val_fold, y_val_fold)\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            train_losses = []  \n",
    "            val_losses = []    \n",
    "\n",
    "            val_f1 = []\n",
    "            train_f1 = []\n",
    "\n",
    "            best_val_accuracy = 0.0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "\n",
    "                progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}')\n",
    "                train_f1_scores = []  \n",
    "                val_f1_scores = []  \n",
    "                \n",
    "                #Train batches\n",
    "                for batch_idx, (inputs, labels) in progress_bar:\n",
    "                    inputs, labels = inputs.to(device), labels.argmax(dim=1).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    f1_batch = f1_score(labels.cpu(), predicted.cpu(), average='weighted')\n",
    "                    train_f1_scores.append(f1_batch)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    progress_bar.set_postfix(loss=loss.item(), f1=f1_batch)\n",
    "\n",
    "                train_f1.append(sum(train_f1_scores) / len(train_f1_scores))\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "\n",
    "                    val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f'Fold {fold + 1}, Validation Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "                    #Validate batches\n",
    "                    for val_batch_idx, (val_inputs, val_labels) in val_progress_bar:\n",
    "                        val_inputs, val_labels = val_inputs.to(device), val_labels.argmax(dim=1).to(device)\n",
    "                        val_outputs = model(val_inputs)\n",
    "                        val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "                        _, predicted_val = torch.max(val_outputs, 1)\n",
    "                        f1_batch_val = f1_score(val_labels.cpu(), predicted_val.cpu(), average='weighted')\n",
    "                        val_f1_scores.append(f1_batch_val)\n",
    "\n",
    "                        _, predicted = torch.max(val_outputs, 1)\n",
    "                        total += val_labels.size(0)\n",
    "                        correct += (predicted == val_labels).sum().item()\n",
    "\n",
    "                    accuracy = correct / total\n",
    "                    val_accuracies.append(accuracy)\n",
    "                    avg_val_f1 = sum(val_f1_scores) / len(val_f1_scores)\n",
    "                    \n",
    "                    print(f'Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, '\n",
    "                        f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {accuracy:.4f}, '\n",
    "                        f'Validation F1 Score: {avg_val_f1:.4f}')\n",
    "\n",
    "                    #Store metrics\n",
    "                    train_losses.append(loss.item())\n",
    "                    val_losses.append(val_loss / len(val_loader))\n",
    "                    val_f1.append(avg_val_f1)\n",
    "\n",
    "                    # Early stopping\n",
    "                    if accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = accuracy\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "\n",
    "\n",
    "                if early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping as validation accuracy is not increasing.\")\n",
    "                    break\n",
    "\n",
    "            all_train_losses.append(train_losses)\n",
    "            all_val_losses.append(val_losses)\n",
    "            all_train_f1_scores.append(train_f1)\n",
    "            all_val_f1_scores.append(val_f1)\n",
    "\n",
    "            print(f\"Learning Rate: {lr}, Best Validation Accuracy: {max(val_accuracies):.4f}\")\n",
    "\n",
    "            epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "        # Plotting the losses for all folds\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for fold in range(num_folds):\n",
    "            plt.plot(epochs_range, all_train_losses[fold], label=f'Fold {fold + 1} Training Loss', linestyle='--')\n",
    "            plt.plot(epochs_range, all_val_losses[fold], label=f'Fold {fold + 1} Validation Loss')\n",
    "\n",
    "        plt.title('Training and Validation Loss Over Epochs for all Folds')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "       # Plotting the F1 scores for all folds\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for fold in range(num_folds):\n",
    "            plt.plot(epochs_range, all_train_f1_scores[fold], label=f'Fold {fold + 1} Training F1 Score', linestyle='--')\n",
    "            plt.plot(epochs_range, all_val_f1_scores[fold], label=f'Fold {fold + 1} Average Validation F1 Score')  # Change label\n",
    "\n",
    "        plt.title('Training and Validation F1 Score Over Epochs for all Folds')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold_lstm_lr(LSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, hidden_dim2=64, output_dim=num_classes, dropout=0.5))\n",
    "# kfold_lstm_lr(SimpleLSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, output_dim=num_classes, dropout=0.5))\n",
    "# kfold_lstm_lr(ModerateLSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, output_dim=num_classes, dropout=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function specifically for testing different number of epochs (by default [8, 12, 16, 20])\n",
    "def kfold_lstm_epochs(a_model, lr=0.0005, epochs=[8, 12, 16, 20]):\n",
    "    #use different learning rates for comparison\n",
    "    device = \"cpu\"\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    all_train_losses = []  \n",
    "    all_val_losses = []    \n",
    "    all_train_f1_scores = []  \n",
    "    all_val_f1_scores = []    \n",
    "    early_stopping_patience = 2  \n",
    "\n",
    "    for num_epochs in epochs:\n",
    "        print(f\"Testing Epochs: {num_epochs} With Learning Rate: {lr}\")\n",
    "\n",
    "        val_accuracies = []\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(X_train)):\n",
    "\n",
    "            model = a_model\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            \n",
    "            print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "            #Split data by specific folds\n",
    "            X_train_fold, y_train_fold = X_train[train_indices], y_train[train_indices]\n",
    "            X_val_fold, y_val_fold = X_train[val_indices], y_train[val_indices]\n",
    "\n",
    "            early_stopping_counter = 0\n",
    "            \n",
    "            train_loader = create_data_loader(X_train_fold, y_train_fold)\n",
    "            val_loader = create_data_loader(X_val_fold, y_val_fold)\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            train_losses = []  \n",
    "            val_losses = []    \n",
    "\n",
    "            val_f1 = []\n",
    "            train_f1 = []\n",
    "\n",
    "            best_val_accuracy = 0.0\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "\n",
    "                progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}')\n",
    "                train_f1_scores = []  \n",
    "                val_f1_scores = []  \n",
    "                \n",
    "                #Train batches\n",
    "                for batch_idx, (inputs, labels) in progress_bar:\n",
    "                    inputs, labels = inputs.to(device), labels.argmax(dim=1).to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    f1_batch = f1_score(labels.cpu(), predicted.cpu(), average='weighted')\n",
    "                    train_f1_scores.append(f1_batch)\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    progress_bar.set_postfix(loss=loss.item(), f1=f1_batch)\n",
    "\n",
    "                train_f1.append(sum(train_f1_scores) / len(train_f1_scores))\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0.0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "\n",
    "                    val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f'Fold {fold + 1}, Validation Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "                    #Validate batches\n",
    "                    for val_batch_idx, (val_inputs, val_labels) in val_progress_bar:\n",
    "                        val_inputs, val_labels = val_inputs.to(device), val_labels.argmax(dim=1).to(device)\n",
    "                        val_outputs = model(val_inputs)\n",
    "                        val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "                        _, predicted_val = torch.max(val_outputs, 1)\n",
    "                        f1_batch_val = f1_score(val_labels.cpu(), predicted_val.cpu(), average='weighted')\n",
    "                        val_f1_scores.append(f1_batch_val)\n",
    "\n",
    "                        _, predicted = torch.max(val_outputs, 1)\n",
    "                        total += val_labels.size(0)\n",
    "                        correct += (predicted == val_labels).sum().item()\n",
    "\n",
    "                    accuracy = correct / total\n",
    "                    val_accuracies.append(accuracy)\n",
    "                    avg_val_f1 = sum(val_f1_scores) / len(val_f1_scores)\n",
    "                    \n",
    "                    print(f'Fold {fold + 1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}, '\n",
    "                        f'Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {accuracy:.4f}, '\n",
    "                        f'Validation F1 Score: {avg_val_f1:.4f}')\n",
    "\n",
    "                    #Store metrics\n",
    "                    train_losses.append(loss.item())\n",
    "                    val_losses.append(val_loss / len(val_loader))\n",
    "                    val_f1.append(avg_val_f1)\n",
    "\n",
    "                    # Early stopping\n",
    "                    if accuracy > best_val_accuracy:\n",
    "                        best_val_accuracy = accuracy\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        early_stopping_counter += 1\n",
    "\n",
    "\n",
    "                if early_stopping_counter >= early_stopping_patience:\n",
    "                    print(\"Early stopping as validation accuracy is not increasing.\")\n",
    "                    break\n",
    "\n",
    "            all_train_losses.append(train_losses)\n",
    "            all_val_losses.append(val_losses)\n",
    "            all_train_f1_scores.append(train_f1)\n",
    "            all_val_f1_scores.append(val_f1)\n",
    "\n",
    "            print(f\"Epochs: {num_epochs}, Learning Rate: {lr}, Best Validation Accuracy: {max(val_accuracies):.4f}\")\n",
    "\n",
    "            epochs_range = range(1, num_epochs + 1)\n",
    "\n",
    "        # Plotting the losses for all folds\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for fold in range(num_folds):\n",
    "            plt.plot(epochs_range, all_train_losses[fold], label=f'Fold {fold + 1} Training Loss', linestyle='--')\n",
    "            plt.plot(epochs_range, all_val_losses[fold], label=f'Fold {fold + 1} Validation Loss')\n",
    "\n",
    "        plt.title('Training and Validation Loss Over Epochs for all Folds')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "       # Plotting the F1 scores for all folds\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for fold in range(num_folds):\n",
    "            plt.plot(epochs_range, all_train_f1_scores[fold], label=f'Fold {fold + 1} Training F1 Score', linestyle='--')\n",
    "            plt.plot(epochs_range, all_val_f1_scores[fold], label=f'Fold {fold + 1} Average Validation F1 Score')  # Change label\n",
    "\n",
    "        plt.title('Training and Validation F1 Score Over Epochs for all Folds')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Epochs: 8 With Learning Rate: 0.0005\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/8: 100%|██████████████████████████████████████| 3902/3902 [04:17<00:00, 15.16it/s, f1=0.326, loss=1.48]\n",
      "Fold 1, Validation Epoch 1/8: 100%|████████████████████████████████████████████████| 3902/3902 [00:40<00:00, 95.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1/8, Loss: 1.4836, Validation Loss: 1.2912, Validation Accuracy: 0.6097, Validation F1 Score: 0.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 2/8:  75%|█████████████████████████████▎         | 2931/3902 [22:45<07:32,  2.15it/s, f1=0.54, loss=1.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m kfold_lstm_epochs(LSTMModel(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(unique_words), embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, hidden_dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, hidden_dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mnum_classes, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m20\u001b[39m])\n\u001b[0;32m      2\u001b[0m kfold_lstm_lr(SimpleLSTMModel(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(unique_words), embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, hidden_dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mnum_classes, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m])\n\u001b[0;32m      3\u001b[0m kfold_lstm_lr(ModerateLSTMModel(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(unique_words), embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, hidden_dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39mnum_classes, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m])\n",
      "Cell \u001b[1;32mIn[20], line 61\u001b[0m, in \u001b[0;36mkfold_lstm_epochs\u001b[1;34m(a_model, lr, epochs)\u001b[0m\n\u001b[0;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     60\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m f1_batch \u001b[38;5;241m=\u001b[39m f1_score(labels\u001b[38;5;241m.\u001b[39mcpu(), predicted\u001b[38;5;241m.\u001b[39mcpu(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m train_f1_scores\u001b[38;5;241m.\u001b[39mappend(f1_batch)\n\u001b[0;32m     64\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1071\u001b[0m     {\n\u001b[0;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1097\u001b[0m ):\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \n\u001b[0;32m   1100\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fbeta_score(\n\u001b[0;32m   1239\u001b[0m         y_true,\n\u001b[0;32m   1240\u001b[0m         y_pred,\n\u001b[0;32m   1241\u001b[0m         beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1242\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1243\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1244\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1245\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1246\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1247\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1251\u001b[0m     {\n\u001b[0;32m   1252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m ):\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \n\u001b[0;32m   1282\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m    0.38...\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1412\u001b[0m         y_true,\n\u001b[0;32m   1413\u001b[0m         y_pred,\n\u001b[0;32m   1414\u001b[0m         beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[0;32m   1415\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1416\u001b[0m         pos_label\u001b[38;5;241m=\u001b[39mpos_label,\n\u001b[0;32m   1417\u001b[0m         average\u001b[38;5;241m=\u001b[39maverage,\n\u001b[0;32m   1418\u001b[0m         warn_for\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-score\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[0;32m   1419\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1420\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[0;32m   1421\u001b[0m     )\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1725\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1725\u001b[0m MCM \u001b[38;5;241m=\u001b[39m multilabel_confusion_matrix(\n\u001b[0;32m   1726\u001b[0m     y_true,\n\u001b[0;32m   1727\u001b[0m     y_pred,\n\u001b[0;32m   1728\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1729\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1730\u001b[0m     samplewise\u001b[38;5;241m=\u001b[39msamplewise,\n\u001b[0;32m   1731\u001b[0m )\n\u001b[0;32m   1732\u001b[0m tp_sum \u001b[38;5;241m=\u001b[39m MCM[:, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1733\u001b[0m pred_sum \u001b[38;5;241m=\u001b[39m tp_sum \u001b[38;5;241m+\u001b[39m MCM[:, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:545\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    542\u001b[0m     tp_bins_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tp_bins):\n\u001b[1;32m--> 545\u001b[0m     tp_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(\n\u001b[0;32m    546\u001b[0m         tp_bins, weights\u001b[38;5;241m=\u001b[39mtp_bins_weights, minlength\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m    547\u001b[0m     )\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;66;03m# Pathological case\u001b[39;00m\n\u001b[0;32m    550\u001b[0m     true_sum \u001b[38;5;241m=\u001b[39m pred_sum \u001b[38;5;241m=\u001b[39m tp_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(labels))\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mbincount\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kfold_lstm_epochs(LSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, hidden_dim2=64, output_dim=num_classes, dropout=0.5), lr=0.0005, epochs=[8, 12, 16, 20])\n",
    "kfold_lstm_lr(SimpleLSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, output_dim=num_classes, dropout=0.5), lr=0.0005, epochs=[32, 32, 32])\n",
    "kfold_lstm_lr(ModerateLSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, output_dim=num_classes, dropout=0.5), lr=0.0005, epochs=[64, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, X_test_tensor):\n",
    "    # Model, Loss, and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    # Training\n",
    "    num_epochs = 64\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    val_accuracies = []\n",
    "    train_losses = []  # Track training losses\n",
    "    val_losses = []    # Track validation losses\n",
    "\n",
    "    patience = 5\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    counter = 0  # Counter for early stopping\n",
    "\n",
    "    tmp_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        #Train batches\n",
    "        for batch_idx, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.argmax(dim=1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=f'Validation Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            #Validate batches\n",
    "            for val_batch_idx, (val_inputs, val_labels) in val_progress_bar:\n",
    "                val_inputs, val_labels = val_inputs.to(device), val_labels.argmax(dim=1).to(device)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "\n",
    "                batch_loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "                y_true.extend(val_labels.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}')\n",
    "\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            val_losses.append(val_loss / len(val_loader))\n",
    "            val_accuracies.append(accuracy)\n",
    "\n",
    "            # Check for early stopping\n",
    "            if accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = accuracy\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "\n",
    "            tmp_epoch = epoch + 1\n",
    "            \n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping as validation accuracy has not improved for {patience} consecutive epochs.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    epochs_range = range(1, tmp_epoch + 1)\n",
    "    \n",
    "    #Plot losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Metrics Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #Plot accuray\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #Generate predictions\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test_tensor)\n",
    "\n",
    "    #Convert the model outputs to predicted labels\n",
    "    _, predicted_labels = torch.max(test_outputs, 1)\n",
    "\n",
    "    #Create a DataFrame for submission\n",
    "    submission_df = pd.DataFrame({\n",
    "        'PhraseId': test['PhraseId'],\n",
    "        'Sentiment': predicted_labels.cpu().numpy()\n",
    "    })\n",
    "\n",
    "    #Save the submission DataFrame to a CSV file\n",
    "    submission_df.to_csv('submission3.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Epoch 1/64:   5%|▍         | 184/3902 [00:04<01:38, 37.80it/s, loss=1.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Final model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m ModerateLSTMModel(vocab_size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(unique_words), embedding_dim\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m, hidden_dim1\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, output_dim\u001b[39m=\u001b[39mnum_classes, dropout\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train_model(model, train_loader, val_loader, X_test)\n",
      "\u001b[1;32m/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb Cell 22\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     lstm_out1, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm1(embedded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     lstm_out1 \u001b[39m=\u001b[39m lstm_out1[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dulatrakhymkul/Desktop/sentiment-analysis-on-movie-reviews/Untitled-1.ipynb#X26sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     fc1_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(lstm_out1)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    880\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    881\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Final model\n",
    "model = ModerateLSTMModel(vocab_size=len(unique_words), embedding_dim=300, hidden_dim1=128, output_dim=num_classes, dropout=0.5)\n",
    "train_model(model, train_loader, val_loader, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
